# Task 15: GGUF Direct Support - Hibrit AI Sistemi

## Hedef

LM Studio/Ollama olmadan GGUF dosyalarÄ±nÄ± direkt Ã§alÄ±ÅŸtÄ±rma desteÄŸi ekle.

## YapÄ±lanlar

### 1. Rust Backend (GGUF Manager)

**Dosyalar:**
- âœ… `src-tauri/Cargo.toml` - llama-cpp-2 dependency eklendi
- âœ… `src-tauri/src/gguf_manager.rs` - GGUF model manager
- âœ… `src-tauri/src/commands.rs` - Tauri commands
- âœ… `src-tauri/src/main.rs` - Module ve state eklendi

**Ã–zellikler:**
- Model yÃ¼kleme (load_gguf_model)
- Chat (chat_with_gguf_model)
- Model unload (unload_gguf_model)
- Status kontrolÃ¼ (get_gguf_model_status)

### 2. TypeScript Frontend (GGUF Provider)

**Dosyalar:**
- âœ… `src/services/ggufProvider.ts` - GGUF API wrapper
- âœ… `src/components/AISettings.tsx` - GGUF provider eklendi

**Ã–zellikler:**
- Model yÃ¼kleme
- Chat
- Dosya seÃ§me (dialog)
- Status kontrolÃ¼

### 3. Hibrit Sistem

ArtÄ±k 3 mod destekleniyor:

**1. LM Studio (Mevcut)**
```
Icon: ğŸ–¥ï¸
URL: http://localhost:1234/v1
Durum: Aktif
```

**2. Ollama (Yeni)**
```
Icon: ğŸ¦™
URL: http://localhost:11434/v1
Durum: Pasif
```

**3. GGUF Direct (Yeni!)**
```
Icon: ğŸ“¦
URL: internal://gguf
Durum: Pasif
Ã–zellik: LM Studio/Ollama gerekmez!
```

## KullanÄ±m

### Model YÃ¼kle:
```typescript
import { loadGgufModel } from './ggufProvider';

await loadGgufModel({
  modelPath: "/path/to/model.gguf",
  contextLength: 32768,
  gpuLayers: 35,
  temperature: 0.7,
  maxTokens: 512
});
```

### Chat:
```typescript
import { chatWithGgufModel } from './ggufProvider';

const response = await chatWithGgufModel(
  "Merhaba",
  512,  // max tokens
  0.7   // temperature
);
```

## Avantajlar

âœ… LM Studio/Ollama gerekmez
âœ… Tek executable
âœ… Daha hÄ±zlÄ± baÅŸlangÄ±Ã§
âœ… Daha fazla kontrol
âœ… Offline Ã§alÄ±ÅŸÄ±r

## Dezavantajlar

âŒ Daha karmaÅŸÄ±k build
âŒ Daha bÃ¼yÃ¼k executable (~50-100 MB)
âŒ Ä°lk yÃ¼kleme yavaÅŸ (5-10 saniye)

## Build Durumu

â³ Build devam ediyor...
- llama.cpp compile ediliyor
- Ä°lk build: 5-10 dakika
- Sonraki buildler: 1-2 dakika

## Sonraki AdÄ±mlar

1. â³ Build tamamlanmasÄ±nÄ± bekle
2. ğŸ”œ UI'da GGUF model seÃ§me ekranÄ± ekle
3. ğŸ”œ Model yÃ¼kleme progress bar
4. ğŸ”œ GPU layer ayarÄ± UI'da
5. ğŸ”œ Context length ayarÄ± UI'da

## DokÃ¼mantasyon

- `GGUF-DIRECT-SUPPORT.md` - DetaylÄ± teknik dokÃ¼mantasyon

## Test

Build tamamlandÄ±ktan sonra:

```bash
# Rust testleri
cd src-tauri
cargo test

# Uygulama Ã§alÄ±ÅŸtÄ±r
npm run tauri dev
```

## Notlar

- llama.cpp compile sÃ¼resi uzun (ilk build)
- GPU desteÄŸi iÃ§in CUDA/Metal/Vulkan gerekli
- Model dosyasÄ± bÃ¼yÃ¼k (7B: ~4GB)
- Ä°lk yÃ¼kleme yavaÅŸ ama sonrasÄ± hÄ±zlÄ±
