# Task 16: GPU Unload ve Context Length DÃ¼zeltmeleri

## Tarih: 2026-02-01

## YapÄ±lan DeÄŸiÅŸiklikler

### 1. GPU'dan Model KaldÄ±rma (Unload) Ã–zelliÄŸi

**Problem**: Model GPU'ya yÃ¼klendiÄŸinde, uygulamadan kaldÄ±rÄ±lsa bile GPU'da kalÄ±yordu. FarklÄ± bir model yÃ¼klemek iÃ§in uygulamayÄ± yeniden baÅŸlatmak gerekiyordu.

**Ã‡Ã¶zÃ¼m**:
- `unloadFromGPU()` fonksiyonu eklendi
- GPU'da aktif model takibi iÃ§in `activeGpuModel` state'i eklendi
- "ğŸ® GPU'dan KaldÄ±r" butonu eklendi (sadece model GPU'da yÃ¼klÃ¼yken gÃ¶rÃ¼nÃ¼r)
- Model listesinde GPU'da aktif olan model ğŸ® ikonu ile iÅŸaretleniyor

**KullanÄ±m**:
1. Model GPU'ya yÃ¼klendiÄŸinde "ğŸ® GPU'dan KaldÄ±r" butonu gÃ¶rÃ¼nÃ¼r
2. Butona tÄ±klandÄ±ÄŸÄ±nda:
   - Model GPU'dan kaldÄ±rÄ±lÄ±r
   - `gguf-active-model` localStorage'dan silinir
   - AI Settings'de GGUF modelleri pasif yapÄ±lÄ±r
   - BaÅŸka bir model yÃ¼klenebilir

**Dosyalar**:
- `local-ai/src/components/GGUFModelBrowser.tsx`
  - `unloadFromGPU()` fonksiyonu eklendi (satÄ±r ~430)
  - `activeGpuModel` state eklendi
  - GPU status kontrolÃ¼ `useEffect`'e eklendi
  - UI'da unload butonu eklendi (hem Ã¼stte hem Model AyarlarÄ± panelinde)
  - Buton her zaman gÃ¶rÃ¼nÃ¼r (model yoksa disabled)

### 2. Context Length AyarÄ± - Preset Butonlar ile Kolay SeÃ§im

**Problem**: 
- Slider ile rastgele deÄŸerler seÃ§ilebiliyordu (8888, 105984 gibi)
- KullanÄ±cÄ± hangi deÄŸeri seÃ§eceÄŸini bilemiyordu
- LM Studio ve Ollama'da Max Tokens dÃ¼zenlenebiliyordu
- GGUF'ta context length sabit kalÄ±yordu

**Ã‡Ã¶zÃ¼m**:
- Slider kaldÄ±rÄ±ldÄ±, yerine **6 preset buton** eklendi
- Her buton optimize edilmiÅŸ deÄŸerler iÃ§eriyor
- GÃ¶rsel geri bildirim ve aÃ§Ä±klamalar eklendi

**Preset DeÄŸerler**:
| Buton | DeÄŸer | AÃ§Ä±klama | KullanÄ±m |
|-------|-------|----------|----------|
| 4K âš¡ | 4096 | HÄ±zlÄ± | KÄ±sa konuÅŸmalar, hÄ±zlÄ± baÅŸlatma |
| 8K âœ… | 8192 | Standart | Normal kullanÄ±m, dengeli |
| 16K ğŸ“š | 16384 | Uzun | Uzun konuÅŸmalar |
| 32K ğŸ”¥ | 32768 | Ã‡ok Uzun | BÃ¼yÃ¼k dosya analizi |
| 64K ğŸ’ª | 65536 | Maksimum | Ã‡ok uzun context |
| 128K ğŸš€ | 131072 | Ultra | Maksimum baÄŸlam (yÃ¼ksek VRAM) |

**AkÄ±ÅŸ**:
1. GGUF Model Browser â†’ Model seÃ§
2. Model AyarlarÄ± panelinde context length slider'Ä± ayarla (Ã¶rn: 8192)
3. "AyarlarÄ± Uygula ve Kullan" butonuna bas
4. Model GPU'ya `n_ctx=8192` ile yÃ¼klenir
5. Terminal'de `n_ctx=8192` gÃ¶rÃ¼nÃ¼r

**Dosyalar**:
- `local-ai/src/components/GGUFModelBrowser.tsx`
  - Context length input'u dÃ¼zenlenebilir yapÄ±ldÄ±
  - AI Settings'den okuma kaldÄ±rÄ±ldÄ±
  - `applyModelConfig()` basitleÅŸtirildi
  - `contextLength` state'i direkt kullanÄ±lÄ±yor

## KarÅŸÄ±laÅŸtÄ±rma: LM Studio vs GGUF

| Ã–zellik | LM Studio | GGUF (Direkt) |
|---------|-----------|---------------|
| Context Length | AI Settings'den dÃ¼zenlenebilir | Model Browser'dan dÃ¼zenlenebilir |
| GPU Unload | Otomatik (LM Studio kapatÄ±lÄ±nca) | Manuel buton ile |
| Model DeÄŸiÅŸtirme | LM Studio'da deÄŸiÅŸtir | GPU'dan kaldÄ±r â†’ Yeni model yÃ¼kle |
| Ayar Yeri | AI AyarlarÄ± â†’ Modeller | GGUF Model Browser â†’ Model AyarlarÄ± |

## Test SenaryolarÄ±

### GPU Unload Testi:
1. âœ… GGUF model GPU'ya yÃ¼kle
2. âœ… "ğŸ® GPU'dan KaldÄ±r" butonunun gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ doÄŸrula
3. âœ… Butona tÄ±kla
4. âœ… Model GPU'dan kaldÄ±rÄ±ldÄ±ÄŸÄ±nÄ± doÄŸrula
5. âœ… BaÅŸka bir model yÃ¼kle (uygulama yeniden baÅŸlatmadan)

### Context Length Testi:
1. âœ… GGUF Model Browser â†’ Model seÃ§
2. âœ… Context Length slider'Ä± 8192'ye ayarla
3. âœ… "AyarlarÄ± Uygula ve Kullan" butonuna bas
4. âœ… Terminal'de `n_ctx=8192` gÃ¶rÃ¼nmeli
5. âœ… 16384, 32768, 65536 deÄŸerleri ile test et

### FarklÄ± Modeller FarklÄ± Context Testi:
1. âœ… Model A'yÄ± 4096 context ile yÃ¼kle
2. âœ… GPU'dan kaldÄ±r
3. âœ… Model B'yi 16384 context ile yÃ¼kle
4. âœ… Her model kendi context'i ile Ã§alÄ±ÅŸmalÄ±

## Teknik Detaylar

### Context Length Slider:
```typescript
<input
  type="range"
  min="512"
  max="131072"
  step="512"
  value={contextLength}
  onChange={(e) => setContextLength(parseInt(e.target.value))}
  className="w-full"
/>
```

### BasitleÅŸtirilmiÅŸ Context KullanÄ±mÄ±:
```typescript
const applyModelConfig = async () => {
  // Direkt UI'daki deÄŸeri kullan
  const aiSettingsMaxTokens = contextLength; // 8192
  
  const ggufConfig = {
    contextLength: aiSettingsMaxTokens,
    ...
  };
  
  await loadGgufModel({
    contextLength: aiSettingsMaxTokens,
    maxTokens: aiSettingsMaxTokens
  });
};
```

## Bilinen SÄ±nÄ±rlamalar

1. Context length maksimum 128K tokens
2. YÃ¼ksek context (>32K) Ã§ok VRAM gerektirir
3. GPU unload iÅŸlemi birkaÃ§ saniye sÃ¼rebilir
4. Her model deÄŸiÅŸikliÄŸinde GPU'dan kaldÄ±rma gerekli

## KullanÄ±cÄ± Deneyimi Ä°yileÅŸtirmeleri

### Ã–ncesi:
- âŒ Context length AI Settings'den ayarlanÄ±yordu (karmaÅŸÄ±k)
- âŒ GGUF iÃ§in sabit kalÄ±yordu
- âŒ Her model iÃ§in aynÄ± context kullanÄ±lÄ±yordu

### SonrasÄ±:
- âœ… Context length model ayarlarÄ± panelinde (kolay eriÅŸim)
- âœ… Slider ile hÄ±zlÄ± ayarlama
- âœ… Her model kendi context'i ile yÃ¼klenebilir
- âœ… GÃ¶rsel geri bildirim (emoji + aÃ§Ä±klama)

## Sonraki AdÄ±mlar

- [ ] Context length'i model yÃ¼kleme sÄ±rasÄ±nda dinamik olarak ayarlama
- [ ] GPU memory kullanÄ±mÄ±nÄ± gÃ¶sterme
- [ ] Ã–nerilen context length hesaplama (model boyutuna gÃ¶re)
- [ ] Context length presets (HÄ±zlÄ±: 2K, Standart: 8K, Uzun: 32K)

## Ä°lgili Dosyalar

- `local-ai/src/components/GGUFModelBrowser.tsx` - Ana deÄŸiÅŸiklikler
- `local-ai/src/services/ggufProvider.ts` - `unloadGgufModel()` fonksiyonu
- `local-ai/src/services/aiProvider.ts` - Context kullanÄ±mÄ±

## Notlar

- Context length artÄ±k her model iÃ§in ayrÄ± ayrÄ± ayarlanabilir
- LM Studio ve Ollama ile aynÄ± kullanÄ±cÄ± deneyimi saÄŸlandÄ±
- GPU unload Ã¶zelliÄŸi model deÄŸiÅŸtirmeyi kolaylaÅŸtÄ±rdÄ±
- Slider kullanÄ±mÄ± daha sezgisel ve hÄ±zlÄ±
