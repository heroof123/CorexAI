// AI Provider Management Service
import { invoke } from "@tauri-apps/api/core";

export interface AIProvider {
  id: string;
  name: string;
  type: 'openai' | 'anthropic' | 'local' | 'custom';
  baseUrl: string;
  host?: string;
  port?: number;
  apiKey?: string;
  models: AIModel[];
  isActive: boolean;
  icon: string;
  description: string;
}

export interface AIModel {
  id: string;
  name: string;
  displayName: string;
  description: string;
  specialty: string;
  roles?: ('coder' | 'tester' | 'planner' | 'chat' | 'reviewer' | 'analyzer')[]; // ğŸ†• Ã‡oklu roller
  maxTokens?: number;
  temperature?: number;
  isActive: boolean;
}

// AI Provider'larÄ± yÃ¼kle
export function loadAIProviders(): AIProvider[] {
  const saved = localStorage.getItem('corex-ai-providers');
  if (saved) {
    try {
      return JSON.parse(saved);
    } catch (error) {
      console.error('AI providers yÃ¼klenemedi:', error);
    }
  }
  
  // Default providers
  return [
    {
      id: "lm-studio",
      name: "LM Studio",
      type: "local",
      baseUrl: "http://127.0.0.1:1234/v1",
      host: "127.0.0.1",
      port: 1234,
      models: [
        {
          id: "main",
          name: "qwen2.5-coder-7b-instruct",
          displayName: "Qwen2.5 Coder 7B",
          description: "Ana model - Planlama ve kodlama",
          specialty: "Coder", // Rollere gÃ¶re gÃ¼ncellendi
          roles: ["coder"], // ğŸ†• Ã‡oklu roller
          maxTokens: 4096,
          temperature: 0.5,
          isActive: true
        },
        {
          id: "chat",
          name: "qwen2.5-3b-instruct",
          displayName: "Qwen2.5 3B",
          description: "HÄ±zlÄ± sohbet ve basit gÃ¶revler",
          specialty: "Chat", // Rollere gÃ¶re gÃ¼ncellendi
          roles: ["chat"], // ğŸ†• Ã‡oklu roller
          maxTokens: 8192, // ğŸ”¥ 2048'den 8192'ye Ã§Ä±karÄ±ldÄ± - kod yazarken yeterli olsun
          temperature: 0.7,
          isActive: true
        }
      ],
      isActive: true,
      icon: "ğŸ–¥ï¸",
      description: "Yerel LM Studio sunucusu"
    }
  ];
}

// AI Provider'larÄ± kaydet
export function saveAIProviders(providers: AIProvider[]): void {
  localStorage.setItem('corex-ai-providers', JSON.stringify(providers));
}

// Aktif modeli bul
export function findActiveModel(modelId: string): { provider: AIProvider; model: AIModel } | null {
  const providers = loadAIProviders();
  
  for (const provider of providers) {
    if (!provider.isActive) continue;
    
    const model = provider.models.find(m => m.id === modelId && m.isActive);
    if (model) {
      return { provider, model };
    }
  }
  
  return null;
}

// Dinamik AI Ã§aÄŸrÄ±sÄ± - provider ayarlarÄ±nÄ± kullanarak
// ğŸ†• Mesajdan resimleri parse et
function parseImagesFromMessage(message: string): { cleanMessage: string; images: string[] } {
  const imageRegex = /\[IMAGES:(\d+)\]\n((?:\[IMAGE_\d+\]:data:image\/[^;]+;base64,[^\n]+\n)+)/;
  const match = message.match(imageRegex);
  
  if (!match) {
    return { cleanMessage: message, images: [] };
  }
  
  const imageCount = parseInt(match[1]);
  const imagesBlock = match[2];
  const images: string[] = [];
  
  // Her bir resmi parse et
  const imageLines = imagesBlock.split('\n').filter(line => line.startsWith('[IMAGE_'));
  for (const line of imageLines) {
    const imageMatch = line.match(/\[IMAGE_\d+\]:(data:image\/[^;]+;base64,.+)/);
    if (imageMatch) {
      images.push(imageMatch[1]);
    }
  }
  
  // Mesajdan resim bloÄŸunu Ã§Ä±kar
  const cleanMessage = message.replace(imageRegex, '').trim();
  
  console.log('ğŸ“· Parse edildi:', { imageCount, foundImages: images.length, cleanMessageLength: cleanMessage.length });
  
  return { cleanMessage, images };
}

// ğŸ†• Conversation history desteÄŸi eklendi
export async function callAI(
  message: string, 
  modelId: string, 
  conversationHistory?: Array<{ role: string; content: string }>,
  onStreamToken?: (text: string) => void // ğŸ†• Streaming callback
): Promise<string> {
  // ğŸ†• Mesajdan resimleri parse et
  const { cleanMessage, images } = parseImagesFromMessage(message);
  
  if (images.length > 0) {
    console.log('ğŸ“· Vision mode aktif:', images.length, 'resim bulundu');
  }
  
  // ğŸ”§ Model ID yoksa veya "default" ise, aktif bir model seÃ§
  let actualModelId = modelId;
  if (!modelId || modelId === 'default') {
    console.log('âš ï¸ Model ID belirtilmemiÅŸ, aktif model aranÄ±yor...');
    const providers = loadAIProviders();
    
    // Ä°lk aktif provider'Ä±n ilk aktif modelini bul
    for (const provider of providers) {
      if (!provider.isActive) continue;
      
      const activeModel = provider.models.find(m => m.isActive);
      if (activeModel) {
        actualModelId = activeModel.id;
        console.log(`âœ… Aktif model bulundu: ${activeModel.displayName} (${actualModelId})`);
        break;
      }
    }
    
    // Hala model bulunamadÄ±ysa hata ver
    if (!actualModelId || actualModelId === 'default') {
      throw new Error('Aktif AI modeli bulunamadÄ±. LÃ¼tfen AI ayarlarÄ±ndan bir model aktif edin.');
    }
  }
  
  const result = findActiveModel(actualModelId);
  
  if (!result) {
    throw new Error(`Model bulunamadÄ±: ${actualModelId}`);
  }
  
  const { provider, model } = result;
  
  console.log('ğŸ¤– AI Ã§aÄŸrÄ±sÄ± yapÄ±lÄ±yor:', {
    modelId,
    provider: provider.name,
    model: model.displayName,
    baseUrl: provider.baseUrl,
    historyLength: conversationHistory?.length || 0
  });
  
  // ğŸ†• GGUF provider kontrolÃ¼ - baseUrl kontrolÃ¼ yerine provider ID kontrolÃ¼
  console.log('ğŸ” Provider kontrolÃ¼:', { id: provider.id, baseUrl: provider.baseUrl, name: provider.name });
  
  if (provider.id === "gguf-direct" || provider.baseUrl === "internal://gguf") {
    console.log('ğŸ“¦ GGUF provider tespit edildi, direkt GGUF Ã§aÄŸrÄ±sÄ± yapÄ±lÄ±yor...');
    
    // GGUF modeli iÃ§in Ã¶zel iÅŸlem
    const { chatWithGgufModel, getGgufModelStatus } = await import('./ggufProvider');
    
    // GGUF model config'i localStorage'dan al
    const ggufConfig = localStorage.getItem('gguf-active-model');
    if (!ggufConfig) {
      throw new Error('âŒ GGUF model yapÄ±landÄ±rmasÄ± bulunamadÄ±. LÃ¼tfen GGUF Model TarayÄ±cÄ±\'dan bir model seÃ§in ve "AyarlarÄ± Uygula ve Kullan" butonuna basÄ±n.');
    }
    
    const config = JSON.parse(ggufConfig);
    console.log('ğŸ“‹ GGUF Config:', config);
    
    // Model durumunu kontrol et
    const status = await getGgufModelStatus();
    console.log('ğŸ“Š GGUF Model Status:', status);
    
    // Model yÃ¼klÃ¼ deÄŸilse hata ver (model yÃ¼kleme iÅŸlemi applyModelConfig'de yapÄ±lmalÄ±)
    if (!status.loaded || status.model_path !== config.modelPath) {
      throw new Error('âŒ GGUF model GPU\'ya yÃ¼klenmemiÅŸ. LÃ¼tfen GGUF Model TarayÄ±cÄ±\'dan "AyarlarÄ± Uygula ve Kullan" butonuna basarak modeli yÃ¼kleyin.');
    }
    
    console.log('âœ… Model yÃ¼klÃ¼, chat yapÄ±lÄ±yor...');
    
    // Model adÄ±ndan chat template'i belirle
    const modelName = config.modelName?.toLowerCase() || '';
    console.log('ğŸ” Model adÄ±:', modelName);
    
    // ğŸ”¥ Context length'i GGUF config'den al (Model Browser'dan ayarlanan deÄŸer)
    let contextLength = config.contextLength || model.maxTokens || 2048;
    
    // ğŸ”¥ CRITICAL FIX: Context length Ã§ok kÃ¼Ã§Ã¼kse otomatik artÄ±r
    // Kod yazarken minimum 4096 context gerekli
    if (contextLength < 4096) {
      console.warn(`âš ï¸ Context length Ã§ok kÃ¼Ã§Ã¼k (${contextLength}), 4096'ya yÃ¼kseltiliyor...`);
      contextLength = 4096;
    }
    
    console.log('ğŸ“ Context length (GGUF config):', contextLength);
    console.log('ğŸ” Config details:', {
      configContextLength: config.contextLength,
      modelMaxTokens: model.maxTokens,
      finalContextLength: contextLength
    });
    
    let fullPrompt = '';
    
    // Conversation history'yi hazÄ±rla (son 4 mesaj)
    const filteredHistory = conversationHistory
      ? conversationHistory.filter(msg => msg.role !== 'system').slice(-4)
      : [];
    
    // Model tipine gÃ¶re chat template seÃ§
    if (modelName.includes('qwen')) {
      // Qwen2.5 ChatML format: <|im_start|>role\ncontent<|im_end|>
      console.log('ğŸ“ Qwen chat template kullanÄ±lÄ±yor');
      
      // System prompt
      fullPrompt += '<|im_start|>system\nYou are a helpful AI assistant. Respond in Turkish.<|im_end|>\n';
      
      // History
      for (const msg of filteredHistory) {
        const role = msg.role === 'user' ? 'user' : 'assistant';
        fullPrompt += `<|im_start|>${role}\n${msg.content}<|im_end|>\n`;
      }
      
      // Current message
      fullPrompt += `<|im_start|>user\n${cleanMessage}<|im_end|>\n<|im_start|>assistant\n`;
      
    } else if (modelName.includes('llama') && modelName.includes('3')) {
      // Llama 3 format
      console.log('ğŸ“ Llama 3 chat template kullanÄ±lÄ±yor');
      
      fullPrompt += '<|begin_of_text|>';
      
      // System prompt
      fullPrompt += '<|start_header_id|>system<|end_header_id|>\n\nYou are a helpful AI assistant. Respond in Turkish.<|eot_id|>';
      
      // History
      for (const msg of filteredHistory) {
        const role = msg.role === 'user' ? 'user' : 'assistant';
        fullPrompt += `<|start_header_id|>${role}<|end_header_id|>\n\n${msg.content}<|eot_id|>`;
      }
      
      // Current message
      fullPrompt += `<|start_header_id|>user<|end_header_id|>\n\n${cleanMessage}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n`;
      
    } else if (modelName.includes('mistral') || modelName.includes('mixtral')) {
      // Mistral format: [INST] ... [/INST]
      console.log('ğŸ“ Mistral chat template kullanÄ±lÄ±yor');
      
      // Mistral doesn't use system prompt in the same way
      let conversationText = '';
      
      // History
      for (const msg of filteredHistory) {
        if (msg.role === 'user') {
          conversationText += `[INST] ${msg.content} [/INST] `;
        } else {
          conversationText += `${msg.content} `;
        }
      }
      
      // Current message
      conversationText += `[INST] ${message} [/INST]`;
      
      fullPrompt = conversationText;
      
    } else if (modelName.includes('gemma')) {
      // Gemma format
      console.log('ğŸ“ Gemma chat template kullanÄ±lÄ±yor');
      
      fullPrompt += '<start_of_turn>user\n';
      
      // History
      for (const msg of filteredHistory) {
        const role = msg.role === 'user' ? 'user' : 'model';
        fullPrompt += `<start_of_turn>${role}\n${msg.content}<end_of_turn>\n`;
      }
      
      // Current message
      fullPrompt += `<start_of_turn>user\n${cleanMessage}<end_of_turn>\n<start_of_turn>model\n`;
      
    } else if (modelName.includes('phi')) {
      // Phi format
      console.log('ğŸ“ Phi chat template kullanÄ±lÄ±yor');
      
      // System prompt
      fullPrompt += '<|system|>\nYou are a helpful AI assistant. Respond in Turkish.<|end|>\n';
      
      // History
      for (const msg of filteredHistory) {
        const role = msg.role === 'user' ? 'user' : 'assistant';
        fullPrompt += `<|${role}|>\n${msg.content}<|end|>\n`;
      }
      
      // Current message
      fullPrompt += `<|user|>\n${cleanMessage}<|end|>\n<|assistant|>\n`;
      
    } else {
      // Generic/Unknown model - simple format
      console.log('ğŸ“ Generic chat template kullanÄ±lÄ±yor (bilinmeyen model)');
      
      // System prompt
      fullPrompt += 'You are a helpful AI assistant. Respond in Turkish.\n\n';
      
      // History
      for (const msg of filteredHistory) {
        const role = msg.role === 'user' ? 'User' : 'Assistant';
        fullPrompt += `${role}: ${msg.content}\n\n`;
      }
      
      // Current message
      fullPrompt += `User: ${cleanMessage}\n\nAssistant:`;
    }
    
    console.log('ğŸ”µ GGUF chat baÅŸlatÄ±lÄ±yor, prompt uzunluÄŸu:', fullPrompt.length);
    console.log('ğŸ“ Prompt preview:', fullPrompt.substring(0, 300));
    
    // Chat yap - maxTokens generation iÃ§in (Ã¼retilecek token sayÄ±sÄ±)
    // Context length zaten model yÃ¼klenirken ayarlandÄ±
    // ğŸ”¥ FIXED: Minimum 2048 token garanti et, kod yazarken yeterli olsun
    const generationMaxTokens = Math.max(Math.min(contextLength / 2, 8192), 2048); // Min 2048, max 8192
    console.log('ğŸ¯ Generation max tokens:', generationMaxTokens, '(context:', contextLength, ')');
    console.log('ğŸ” Calculation:', {
      contextLength,
      contextHalf: contextLength / 2,
      minWithMax: Math.min(contextLength / 2, 8192),
      finalWithMin: Math.max(Math.min(contextLength / 2, 8192), 2048)
    });
    
    // ğŸ†• Streaming desteÄŸi
    if (onStreamToken) {
      const { chatWithChunkedStreaming } = await import('./streamingProvider');
      const response = await chatWithChunkedStreaming(
        fullPrompt,
        generationMaxTokens,
        model.temperature || 0.7,
        {
          onToken: onStreamToken,
          onComplete: (text) => console.log('âœ… Streaming tamamlandÄ±:', text.length, 'karakter')
        }
      );
      return response;
    }
    
    // Normal (non-streaming) mode
    const response = await chatWithGgufModel(
      fullPrompt,
      generationMaxTokens, // Ãœretilecek token sayÄ±sÄ±
      model.temperature || 0.7
    );
    
    console.log('âœ… GGUF response alÄ±ndÄ±, uzunluk:', response.length);
    return response;
  }
  
  // Normal provider (LM Studio, Ollama, vb.)
  // Timeout ile AI Ã§aÄŸrÄ±sÄ± (60 saniye - daha uzun cevaplar iÃ§in)
  const timeoutPromise = new Promise<never>((_, reject) => {
    setTimeout(() => reject(new Error('AI isteÄŸi zaman aÅŸÄ±mÄ±na uÄŸradÄ± (60 saniye)')), 60000);
  });

  // Temperature'Ä± biraz artÄ±r (daha yaratÄ±cÄ± ve eksiksiz cevaplar iÃ§in)
  const adjustedTemperature = model.temperature ? Math.min(model.temperature + 0.1, 0.9) : 0.7;
  
  // Max tokens'Ä± artÄ±r (daha uzun cevaplar iÃ§in)
  const adjustedMaxTokens = model.maxTokens ? Math.max(model.maxTokens, 8192) : 8192;

  const aiPromise = invoke<string>("chat_with_dynamic_ai", {
    message,
    conversationHistory: conversationHistory || [],
    providerConfig: {
      base_url: provider.baseUrl,
      host: provider.host || null,
      port: provider.port || null,
      api_key: provider.apiKey || null,
      model_name: model.name,
      temperature: adjustedTemperature,
      max_tokens: adjustedMaxTokens
    }
  });

  return await Promise.race([aiPromise, timeoutPromise]);
}

// Provider baÄŸlantÄ±sÄ±nÄ± test et
export async function testProviderConnection(provider: AIProvider): Promise<boolean> {
  try {
    // ğŸ†• GGUF provider iÃ§in Ã¶zel test
    if (provider.id === "gguf-direct" || provider.baseUrl === "internal://gguf") {
      console.log('ğŸ§ª GGUF provider test ediliyor...');
      
      // GGUF model status kontrolÃ¼
      const { getGgufModelStatus } = await import('./ggufProvider');
      const status = await getGgufModelStatus();
      
      console.log('ğŸ“Š GGUF Status:', status);
      
      // Model yÃ¼klÃ¼yse baÅŸarÄ±lÄ±
      if (status.loaded) {
        console.log('âœ… GGUF Test Sonucu: Model yÃ¼klÃ¼ - BAÅARILI');
        return true;
      }
      
      // Model yÃ¼klÃ¼ deÄŸilse ama config varsa uyarÄ± ver
      const hasConfig = localStorage.getItem('gguf-active-model') !== null;
      if (hasConfig) {
        console.log('âš ï¸ GGUF Test Sonucu: Config var ama model yÃ¼klÃ¼ deÄŸil');
        return false;
      }
      
      console.log('âŒ GGUF Test Sonucu: Model yapÄ±landÄ±rÄ±lmamÄ±ÅŸ');
      return false;
    }
    
    // Normal provider test (HTTP)
    const headers: Record<string, string> = {
      'Content-Type': 'application/json'
    };
    
    if (provider.apiKey) {
      headers['Authorization'] = `Bearer ${provider.apiKey}`;
    }
    
    // Timeout ile fetch (5 saniye)
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 5000);
    
    const response = await fetch(`${provider.baseUrl}/models`, {
      method: 'GET',
      headers,
      signal: controller.signal
    });
    
    clearTimeout(timeoutId);
    
    return response.ok;
  } catch (error) {
    console.error('Provider baÄŸlantÄ± testi hatasÄ±:', error);
    return false;
  }
}

// Mevcut modelleri listele (API'den)
export async function fetchAvailableModels(provider: AIProvider): Promise<string[]> {
  try {
    // ğŸ†• GGUF provider iÃ§in Ã¶zel liste
    if (provider.baseUrl === "internal://gguf") {
      console.log('ğŸ“¦ GGUF provider iÃ§in model listesi alÄ±nÄ±yor...');
      
      // localStorage'dan aktif GGUF modelini al
      const ggufConfig = localStorage.getItem('gguf-active-model');
      if (ggufConfig) {
        const config = JSON.parse(ggufConfig);
        console.log('âœ… GGUF Model bulundu:', config.modelName);
        return [config.modelName || 'GGUF Model'];
      }
      
      console.log('âš ï¸ GGUF model yapÄ±landÄ±rÄ±lmamÄ±ÅŸ');
      return [];
    }
    
    // Normal provider (HTTP)
    const headers: Record<string, string> = {
      'Content-Type': 'application/json'
    };
    
    if (provider.apiKey) {
      headers['Authorization'] = `Bearer ${provider.apiKey}`;
    }
    
    console.log('ğŸ” Model listesi alÄ±nÄ±yor:', provider.baseUrl);
    
    // Timeout ile fetch (10 saniye)
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 10000);
    
    const response = await fetch(`${provider.baseUrl}/models`, {
      method: 'GET',
      headers,
      signal: controller.signal
    });
    
    clearTimeout(timeoutId);
    
    console.log('ğŸ“¡ Response status:', response.status);
    
    if (!response.ok) {
      const errorText = await response.text();
      console.error('âŒ API hatasÄ±:', response.status, errorText);
      throw new Error(`HTTP ${response.status}: ${errorText}`);
    }
    
    const data = await response.json();
    console.log('ğŸ“¥ API Response:', data);
    
    // OpenAI format
    if (data.data && Array.isArray(data.data)) {
      const models = data.data.map((model: any) => model.id || model.name).filter(Boolean);
      console.log('âœ… Bulunan modeller:', models);
      return models;
    }
    
    // LM Studio format (bazen direkt array dÃ¶ner)
    if (Array.isArray(data)) {
      const models = data.map((model: any) => model.id || model.name || model).filter(Boolean);
      console.log('âœ… Bulunan modeller (array):', models);
      return models;
    }
    
    // Ollama format
    if (data.models && Array.isArray(data.models)) {
      const models = data.models.map((model: any) => model.name || model.id).filter(Boolean);
      console.log('âœ… Bulunan modeller (ollama):', models);
      return models;
    }
    
    console.warn('âš ï¸ Beklenmeyen API response formatÄ±:', data);
    return [];
  } catch (error) {
    console.error('âŒ Model listesi alÄ±namadÄ±:', error);
    throw error; // HatayÄ± yukarÄ± fÄ±rlat ki kullanÄ±cÄ± gÃ¶rebilsin
  }
}