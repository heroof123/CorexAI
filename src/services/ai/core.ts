import { storage } from '../storage';
import { selectPromptMode, type AutonomyMeta } from '../../prompts/corex_system_prompt';
import { getModelIdForRole } from './models';
import { analyzeUserIntent, generateSummary } from './analysis';
import { conversationContext, pruneHistory, updateConversationContext } from './context';
import { estimateTokens } from './utils';

/**
 * AI'ya mesaj gÃ¶nderen ana fonksiyon.
 * Tool kullanÄ±mÄ±, RAG ve geÃ§miÅŸ yÃ¶netimini koordine eder.
 */
export async function sendToAI(
    message: string,
    resetHistory: boolean = false,
    onToolExecution?: (toolName: string, status: 'running' | 'completed' | 'failed', result?: any, error?: string) => void,
    onToolApprovalRequest?: (toolName: string, parameters: any) => Promise<boolean>
): Promise<string> {
    // Prevent concurrent calls
    if ((sendToAI as any).isProcessing) {
        console.warn("âš ï¸ AI Ã§aÄŸrÄ±sÄ± zaten iÅŸleniyor, yeni Ã§aÄŸrÄ± reddedildi");
        throw new Error("AI Ã§aÄŸrÄ±sÄ± zaten iÅŸleniyor. LÃ¼tfen bekleyin.");
    }

    (sendToAI as any).isProcessing = true;

    try {
        if (resetHistory) {
            conversationContext.history = [];
        }

        // ğŸ†• GGUF model config'inden context ve output limitlerini al
        const config = await storage.getSettings<any>('gguf-active-model');
        if (config) {
            conversationContext.maxContextTokens = config.contextLength || 32768;
            console.log(`ğŸ“ Context limit gÃ¼ncellendi: ${conversationContext.maxContextTokens}`);
        }

        // ğŸ†• Output mode'u localStorage'dan al
        const outputMode = await storage.getSettings<string>('ai-output-mode') || 'normal';
        conversationContext.maxOutputTokens =
            outputMode === 'brief' ? 2048 :
                outputMode === 'detailed' ? 16384 : 8192;

        console.log(`ğŸ“¤ Output limit: ${conversationContext.maxOutputTokens} (${outputMode})`);


        // Analyze user intent and update context
        const userIntent = analyzeUserIntent(message);
        updateConversationContext(message, userIntent);

        // Get tools prompt dynamically (includes MCP tools)
        const { getToolsPrompt } = await import('./aiTools');
        const toolsPrompt = await getToolsPrompt();

        // ğŸ§  CorexA Ultimate System Prompt â€” autonomy + verbosity + proje baÄŸlamÄ±yla
        const { getAutonomyConfig: getAutonomyCfg } = await import('./autonomy');
        const autonomyConfig = getAutonomyCfg();
        const corexMeta: AutonomyMeta = {
            level: autonomyConfig.level as 1 | 2 | 3 | 4 | 5,
            verbosity: outputMode === 'brief' ? 'concise' : outputMode === 'detailed' ? 'detailed' : 'balanced',
            modelName: getModelIdForRole(),
            projectPath: conversationContext.projectContext?.name || undefined,
            currentFile: conversationContext.recentFiles?.[0] || undefined,
        };
        const systemPrompt = selectPromptMode(message, toolsPrompt, corexMeta);
        console.log('ğŸ§  CorexA System Prompt seÃ§ildi (level:', corexMeta.level, '| verbosity:', corexMeta.verbosity, ')');

        // Add system prompt if this is the first message
        if (conversationContext.history.length === 0) {
            conversationContext.history.push({
                role: "system",
                content: systemPrompt,
                timestamp: Date.now(),
                tokens: estimateTokens(systemPrompt)
            });
        }

        // Add user message to history
        const userTokens = estimateTokens(message);
        conversationContext.history.push({
            role: "user",
            content: message,
            timestamp: Date.now(),
            tokens: userTokens
        });

        // ğŸ†• Mesaj sayacÄ±nÄ± artÄ±r
        conversationContext.messagesSinceLastSummary++;

        // ğŸ†• Her 10 mesajda bir Ã¶zet oluÅŸtur
        if (conversationContext.messagesSinceLastSummary >= 10) {
            console.log('ğŸ“ 10 mesaj geÃ§ti, Ã¶zet oluÅŸturuluyor...');

            const summary = await generateSummary(conversationContext.history);

            if (summary) {
                conversationContext.summary = summary;
                conversationContext.messagesSinceLastSummary = 0;

                console.log('âœ… Ã–zet kaydedildi:', summary.substring(0, 100) + '...');
            }
        }

        // ğŸ†• History'yi temizle (context'in %40'Ä± history iÃ§in)
        const maxHistoryTokens = Math.floor(conversationContext.maxContextTokens * 0.4);
        pruneHistory(maxHistoryTokens);

        // ğŸ†• Dinamik AI provider kullan - conversation history ile
        const { callAI } = await import('./aiProvider');
        const modelId = getModelIdForRole();

        // ğŸ†• Ã–zet varsa history'nin baÅŸÄ±na ekle (system prompt'tan sonra)
        let historyWithSummary = [...conversationContext.history];
        if (conversationContext.summary) {
            const summaryMessage = {
                role: 'system',
                content: `ğŸ“ Ã–nceki KonuÅŸma Ã–zeti:\n${conversationContext.summary}\n\n---\n`,
                timestamp: Date.now(),
                tokens: estimateTokens(conversationContext.summary)
            };

            // System prompt'tan sonra, diÄŸer mesajlardan Ã¶nce ekle
            historyWithSummary.splice(1, 0, summaryMessage);
            console.log('ğŸ“Œ Ã–zet history\'ye eklendi');
        }

        // ğŸ§  RAG (VektÃ¶rel Kod HafÄ±zasÄ±) Entegrasyonu
        try {
            const { ragService } = await import('./ragService');
            // KullanÄ±cÄ±nÄ±n mesajÄ±ndaki niyetine gÃ¶re ilk 4 semantik parÃ§ayÄ± bul
            const vectorResults = await ragService.search(message, 4);

            if (vectorResults && vectorResults.length > 0) {
                console.log(`ğŸ” RAG: ${vectorResults.length} adet kod baÄŸlamÄ± hafÄ±zadan Ã§ekildi.`);

                let ragContextText = "ğŸ§  PROJE HAFIZASI (VektÃ¶rel Arama SonuÃ§larÄ±):\n\nBu baÄŸlam sana projenin kod tabanÄ±ndan getirilmiÅŸtir. LÃ¼tfen yanÄ±t verirken aÅŸaÄŸÄ±daki dosyalarÄ±n varlÄ±ÄŸÄ±nÄ± ve iÃ§eriÄŸini bilerek hareket et:\n\n";

                vectorResults.forEach(res => {
                    // Token ÅŸiÅŸmemesi iÃ§in her dosyanÄ±n max 1500 karakterini al
                    ragContextText += `--- DOSYA: ${res.file_path} ---\n\`\`\`\n${res.content.substring(0, 1500)}\n\`\`\`\n\n`;
                });

                // Bu veriyi hafÄ±zayÄ± ÅŸiÅŸirmemek iÃ§in ASIL HISTORY dizisine DEÄÄ°L, sadece bu anlÄ±k isteÄŸe giden historyWithSummary kopyasÄ±na ekliyoruz.
                const ragMessage = {
                    role: "system",
                    content: ragContextText,
                    timestamp: Date.now(),
                    tokens: estimateTokens(ragContextText)
                };

                // KullanÄ±cÄ± mesajÄ±ndan (en son mesaj) hemen Ã¶nce araya yerleÅŸtir
                const userMsgIndex = historyWithSummary.length - 1;
                historyWithSummary.splice(userMsgIndex, 0, ragMessage);
            }
        } catch (ragError) {
            console.warn("âš ï¸ RAG aramasÄ± yapÄ±lamadÄ± (VektÃ¶r DB henÃ¼z hazÄ±r olmayabilir):", ragError);
        }

        // Prepare conversation history for AI (only role and content)
        const historyForAI = historyWithSummary.map(msg => ({
            role: msg.role,
            content: msg.content
        }));

        console.log('ğŸ“¤ AI\'ye gÃ¶nderilen history:', historyForAI.length, 'mesaj');
        console.log('ğŸ“Š Tahmini history token:', conversationContext.history.reduce((sum, msg) => sum + (msg.tokens || 0), 0));

        // Add timeout to prevent hanging (5 minutes for GGUF models)
        const timeoutPromise = new Promise<never>((_, reject) => {
            setTimeout(() => reject(new Error('AI Ã§aÄŸrÄ±sÄ± zaman aÅŸÄ±mÄ±na uÄŸradÄ± (300 saniye)')), 300000);
        });

        let response = await Promise.race([
            callAI(message, modelId, historyForAI), // ğŸ”¥ History ile gÃ¶nder
            timeoutPromise
        ]);

        // ğŸ”§ TOOL SYSTEM - Parse and execute tools
        const { parseToolCalls, executeTool } = await import('./aiTools');
        const { requiresApproval } = await import('./autonomy');

        let toolCalls = parseToolCalls(response);
        let toolIterations = 0;
        const maxToolIterations = 5; // Sonsuz dÃ¶ngÃ¼ Ã¶nleme

        while (toolCalls.length > 0 && toolIterations < maxToolIterations) {
            toolIterations++;
            console.log(`ğŸ”§ Ã‡oklu Tool Ã‡aÄŸrÄ±sÄ± tespit edildi (${toolIterations}/${maxToolIterations}): ${toolCalls.map(t => t.toolName).join(', ')}`);

            const sessionResults: string[] = [];
            for (const toolCall of toolCalls) {
                // ğŸšï¸ AUTONOMY CHECK - Onay gerekli mi? (corexMeta.level zaten yukarÄ±da tanÄ±mlÄ±)
                const config = autonomyConfig;
                const needsApproval = requiresApproval(toolCall.toolName, toolCall.parameters, config);

                let executionResult: any = null;
                let isApproved = true;

                if (needsApproval && onToolApprovalRequest) {
                    console.log('ğŸ” Tool onay gerektiriyor:', toolCall.toolName);
                    const approved = await onToolApprovalRequest(toolCall.toolName, toolCall.parameters);

                    if (!approved) {
                        console.log('âŒ Tool reddedildi:', toolCall.toolName);
                        isApproved = false;
                        executionResult = { success: false, error: 'User rejected the tool execution.' };
                    } else {
                        console.log('âœ… Tool onaylandÄ±:', toolCall.toolName);
                    }
                } else {
                    console.log('ğŸš€ Tool otomatik Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor:', toolCall.toolName);
                }

                if (isApproved) {
                    if (onToolExecution) onToolExecution(toolCall.toolName, 'running');

                    executionResult = await executeTool(toolCall.toolName, toolCall.parameters);
                    console.log(`ğŸ”§ Tool sonucu (${toolCall.toolName}):`, executionResult);

                    if (onToolExecution) {
                        if (executionResult.success) {
                            onToolExecution(toolCall.toolName, 'completed', executionResult);
                        } else {
                            onToolExecution(toolCall.toolName, 'failed', executionResult, executionResult.error);
                        }
                    }
                }

                sessionResults.push(`ğŸ”§ Tool Result (${toolCall.toolName}):\n${JSON.stringify(executionResult, null, 2)}`);
            }

            // TÃ¼m tool sonuÃ§larÄ±nÄ± tek mesaj olarak history'ye ekle
            const combinedToolResultMessage = sessionResults.join('\n\n');
            conversationContext.history.push({
                role: "user",
                content: combinedToolResultMessage,
                timestamp: Date.now(),
                tokens: estimateTokens(combinedToolResultMessage)
            });

            conversationContext.messagesSinceLastSummary++;

            // AI'ya tÃ¼m tool sonuÃ§larÄ±nÄ± gÃ¶nder ve devam et
            const continuePrompt = "AraÃ§lar(Tools) Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±. SonuÃ§larÄ± yukarÄ±da gÃ¶rebilirsin. Duruma gÃ¶re adÄ±m adÄ±m ilerlemeye devam et.";
            const historyForAI2 = conversationContext.history.map(msg => ({
                role: msg.role,
                content: msg.content
            }));

            response = await Promise.race([
                callAI(continuePrompt, modelId, historyForAI2),
                timeoutPromise
            ]);

            // Yeni response'da baÅŸka tool var mÄ± kontrol et
            toolCalls = parseToolCalls(response);
        }

        if (toolIterations >= maxToolIterations) {
            console.warn('âš ï¸ Maksimum tool iterasyonu aÅŸÄ±ldÄ±');
            response += '\n\nâš ï¸ (Maksimum tool Ã§aÄŸrÄ±sÄ± limitine ulaÅŸÄ±ldÄ±)';
        }

        // Add AI response to history
        const responseTokens = estimateTokens(response);
        conversationContext.history.push({
            role: "assistant",
            content: response,
            timestamp: Date.now(),
            tokens: responseTokens
        });

        // ğŸ†• AI cevabÄ± da sayÄ±lÄ±r
        conversationContext.messagesSinceLastSummary++;

        // ğŸ†• Response Ã§ok uzunsa uyar
        if (responseTokens > conversationContext.maxOutputTokens * 0.9) {
            console.warn(`âš ï¸ Cevap Ã§ok uzun: ${responseTokens} token (limit: ${conversationContext.maxOutputTokens})`);
        }

        return response;
    } catch (error) {
        console.error('âŒ AI hatasÄ±:', error);

        // Aktif model bulunamadÄ±ysa kullanÄ±cÄ±ya bildir
        if (error instanceof Error && error.message.includes('Model bulunamadÄ±')) {
            throw new Error('âŒ Aktif AI modeli bulunamadÄ±. LÃ¼tfen AI ayarlarÄ±ndan bir model aktif edin.');
        }

        // BaÄŸlantÄ± hatasÄ± varsa kullanÄ±cÄ±ya bildir
        if (error instanceof Error && (error.message.includes('fetch') || error.message.includes('network'))) {
            throw new Error('âŒ AI sunucusuna baÄŸlanÄ±lamadÄ±. LM Studio veya AI saÄŸlayÄ±cÄ±nÄ±zÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.');
        }

        // Timeout hatasÄ±
        if (error instanceof Error && error.message.includes('zaman aÅŸÄ±mÄ±')) {
            throw new Error('âŒ AI yanÄ±t verme sÃ¼resi aÅŸÄ±ldÄ±. LÃ¼tfen tekrar deneyin.');
        }

        // DiÄŸer hatalar iÃ§in genel mesaj
        throw new Error(`âŒ AI hatasÄ±: ${error instanceof Error ? error.message : 'Bilinmeyen hata'}`);
    } finally {
        (sendToAI as any).isProcessing = false;
    }
}

// Static property initialization
(sendToAI as any).isProcessing = false;

// âš–ï¸ MODEL KARÅILAÅTIRMA MODU
export async function compareModels(
    message: string,
    modelId1: string,
    modelId2: string,
    onToken1?: (token: string, metrics?: { speed: number }) => void,
    onToken2?: (token: string, metrics?: { speed: number }) => void
): Promise<{ response1: string; response2: string; metrics1: any; metrics2: any }> {
    console.log(`âš–ï¸ KarÅŸÄ±laÅŸtÄ±rma baÅŸlatÄ±lÄ±yor: ${modelId1} vs ${modelId2}`);

    const { callAI } = await import('./aiProvider');

    // Ortak history hazÄ±rla
    const historyForAI = conversationContext.history.map(msg => ({
        role: msg.role,
        content: msg.content
    }));

    const start1 = Date.now();
    let tokens1 = 0;
    const promise1 = callAI(message, modelId1, historyForAI, (token: string) => {
        tokens1++;
        const elapsed = (Date.now() - start1) / 1000;
        const speed = elapsed > 0 ? tokens1 / elapsed : 0;
        if (onToken1) onToken1(token, { speed });
    });

    const start2 = Date.now();
    let tokens2 = 0;
    const promise2 = callAI(message, modelId2, historyForAI, (token: string) => {
        tokens2++;
        const elapsed = (Date.now() - start2) / 1000;
        const speed = elapsed > 0 ? tokens2 / elapsed : 0;
        if (onToken2) onToken2(token, { speed });
    });

    const [res1, res2] = await Promise.all([promise1, promise2]);

    const end1 = Date.now();
    const end2 = Date.now();

    const metrics1 = {
        duration: (end1 - start1) / 1000,
        tokens: tokens1,
        speed: tokens1 / ((end1 - start1) / 1000)
    };

    const metrics2 = {
        duration: (end2 - start2) / 1000,
        tokens: tokens2,
        speed: tokens2 / ((end2 - start2) / 1000)
    };

    return {
        response1: res1,
        response2: res2,
        metrics1,
        metrics2
    };
}
